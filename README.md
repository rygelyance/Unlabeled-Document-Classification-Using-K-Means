# Unlabeled Document Classification Using K-Means
This is a demonstration of my data science skills within Python on an unsupervised learning NLP problem. It uses a Sentence Transformer model and an optimized version of the K-Means Algorithm to classify a list of unlabeled documents based on context and meaning.

## Background/Context
The goal behind this project was to solve the problem of classifying a large set of unlabeled documents based on nothing but the words and context contained within the documents themselves! I think this is a really interesting problem and a good introduction to the world of Natrual Language Processing (NLP) and a good opportunity to demonstrate knowledge of topics within unsupervised learning.

## Dataset Source
Somewhat contrary to the problem at hand, the datset I used was a labeled dataset from Kaggle, but the reasons why will become evident later. I used the dataset from https://www.kaggle.com/datasets/sunilthite/text-document-classification-dataset. This dataset contains 2,225 text documents across 5 categories, Politics, Sport, Technology, Entertainment, and Business. In this case I knew the optimal value for K (5), but in a real world case I'd have to experiment using either the Elbow or Silhouette Methods.

## Approach/Method
There are two real parts to this type of classification, a method of vectorizing the words in a way that preserves their context and meaning (word embeddings), and a way of grouping or clustering those embeddings. The first part can be done in several ways, such as Term Frequency - Inverse Document Frequency, but in order to gain even better word embeddings, I elected to use a pretrained Sentence Transformer model from the Sentence Transformers (SBERT) module in Python.

For the second part, I elected to use a customized version of the K-Means Algorithm. The main drawback behind K-Means is in its initialization, if the centroids during initialization aren't in good places, the algorithm may converge on poor clustering. Hence I customized the K-Means algorithm using the recommended optimizations from (Fränti, 2019). This methodology involves using the farthest point heuristic (MaxMin), and repetition of the K-Means algorithm to gain better accuracy. To determine which run of K-Means was the best in the repetitions without using the accuracy (that would be cheating!) I elected to use Inertia as the measurement. Inertia refers to the sum of squared distances between each point and the cluster it belongs to. This can also be thought of as how "compact" the clusters are, and it worked well as a method for determining the quality of the clustering in this case, but was aided by knowing the number of clusters (5) beforehand.

Side Note: I initially tried using a single run on K-Means on this data and would get wildly ranging accuracy (between roughly 70% and 95%) depending on the random initialization of the centroids, indicating a need for better 

## Results/Accuracy
To measure the quality of the labels generated by the Sentence Transformer and K-Means model, I compared the generated labels against the correct labels already within the dataset, using the Hungarian Algorithm to find the best mapping of labels. This is necessary due to the arbitrary nature of the labeling, the K-Means model may label the Politics category as "2" when it is labeled "0" in the original dataset. The algorithm serves to find the best match in the labeling and subsequently produces an accuracy percentage based on how many are correct.

Setting the K-Means Algorithm to repeat 20 times regularly produced an accuracy of over 95%, with the highest accuracy I've personally seen being 95.69%. While the 2019 paper did recommend using 100 repeats to continue to improve the accuracy, for this dataset I found it made very little difference even across multiple runs. (Fränti, 2019) 

I'm overall quite happy with the results produced by this simple project, considering that at its core, all it's given is the documents, no titles, no labels, and was able to place the vast majority of them in the correct categories in seconds.
